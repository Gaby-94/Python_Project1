{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0872f836",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from  sklearn.datasets import make_hastie_10_2\n",
    "X, y = make_hastie_10_2(n_samples=12000, random_state=808)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9ddb2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Réduisez le biais\n",
    "\n",
    "#Regardons comment évolue le score de notre classification du stade de développement en accroissant \n",
    "#le nombre d'arbres. On considère un arbre de profondeur 2 comme modèle de base. Le nombre d'arbres \n",
    "#dans la forêt varie de 1 à 120."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c0a75bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_counts = [1,2,3,4,5,10,15,20,25,30,40,50, 60, 70, 80, 90, 100, 110, 120]\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, train_size=0.8, random_state=8)\n",
    "\n",
    "accuracy  = []\n",
    "for n_estimator in tree_counts:\n",
    "    clf = RandomForestClassifier(\n",
    "        n_estimators = n_estimator,\n",
    "        max_depth = 2,\n",
    "        random_state = 8\n",
    "        )\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    accuracy.append(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60a47ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#La fonction score du  RandomForestClassifier  donne l'accuracy :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d3b6d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Réduisez la variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc991a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Le bagging permet aussi de réduire l'overfit d'un modèle. \n",
    "#Prenons comme modèle de base un arbre dont la profondeur n'est pas contrainte et qui, par conséquent, overfit. \n",
    "#Fixons aussi le paramètre  max_features  de façon à ce que toutes les variables soient prises\n",
    "#en compte dans chaque prédicteur faible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6f87ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "        clf = RandomForestClassifier(\n",
    "            n_estimators = n_estimator,\n",
    "            max_depth = None,\n",
    "            max_features = None,\n",
    "            random_state = 8\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24d842a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'https://raw.githubusercontent.com/OpenClassrooms-Student-Center/8063076-Initiez-vous-au-Machine-Learning/master/data/paris-arbres-numerical-2023-09-10.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25b6bf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(filename)\n",
    "X = data[['domanialite', 'arrondissement','libelle_francais', 'genre', 'espece','circonference_cm', 'hauteur_m']]\n",
    "y = data.stade_de_developpement.values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, train_size=0.8, random_state=808)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32bd3aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#et entraînons une forêt aléatoire "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73d90d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0388565  0.16102324 0.04076572 0.03633334 0.05559153 0.46194169\n",
      " 0.20548799]\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(\n",
    "    n_estimators = 100,\n",
    "    random_state = 8\n",
    "    )\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(clf.feature_importances_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "345a1050",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Découvrez le principe du boosting appliqué aux arbres de décision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d72cd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "learning_rates = [1, 0.6,  0.3, 0.1]\n",
    "for lr in learning_rates:\n",
    "\n",
    "    clf = GradientBoostingClassifier(\n",
    "          n_estimators= 500,\n",
    "          max_depth= 2,\n",
    "          random_state= 8,\n",
    "          learning_rate= lr\n",
    "    )\n",
    "    clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e951a156",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Voici le code pour calculer le  log_loss  à chaque itération et pour chaque valeur du learning rate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40c4b76b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'log_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-5d2450f079ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_proba\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstaged_predict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mlog_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_proba\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'log_loss' is not defined"
     ]
    }
   ],
   "source": [
    "scores = np.zeros((clf.n_estimators,), dtype=np.float64)\n",
    "for i, y_proba in enumerate(clf.staged_predict_proba(X_test)):\n",
    "  scores[i] =  log_loss(y_test, y_proba[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785ae8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#En résumé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9a7daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#    L'apprentissage d'ensemble donne naissance à des modèles robustes et performants moins sujets au sur-apprentissage.\n",
    "\n",
    " #   Les forêts aléatoires consistent à entraîner de multiples arbres de décision en parallèle et à moyenner leurs prédictions. Contraindre la profondeur des arbres correspond à une régularisation qui compense le sur-apprentissage.\n",
    "\n",
    " #   Le Gradient Boosting enchaîne l'entraînement d'arbres de décision en se concentrant sur les erreurs de la séquence. C'est en général le modèle qui  sera le plus efficace, mais il est plus délicat à paramétrer. \n",
    "\n",
    "  #  Définir un benchmark avec une simple régression pour fixer les attentes en termes de performance est une bonne pratique avant de se lancer dans l'entraînement de modèles ensemblistes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
